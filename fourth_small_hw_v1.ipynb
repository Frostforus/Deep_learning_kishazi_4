{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36UW202IWdNP"
   },
   "source": [
    "# Fourth small Homework\n",
    "\n",
    "In this homework I will be getting a file from an online database and use a pretrained network to train it on this database using transfered learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiVzjpPHWtks"
   },
   "source": [
    "I used a different database from the suggested one, but this is a google dataset aswell. The result were promising"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RsnUPmflEfBe",
    "outputId": "c50454e2-49a1-4008-dbe9-5fed8c8cd63e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Downloading the compressed data that we will be using\n",
    "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O /tmp/cats_and_dogs_filtered.zip"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2020-11-11 20:49:54--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.15.80, 172.217.12.240, 172.217.9.208, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.15.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68606236 (65M) [application/zip]\n",
      "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
      "\n",
      "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  34.3MB/s    in 1.9s    \n",
      "\n",
      "2020-11-11 20:49:56 (34.3 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WX1v71r2EfBj"
   },
   "source": [
    "#import the inception model which is my pretrained model of choice \n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "elh1XhfKEfBl",
    "outputId": "8e257667-ceda-4ceb-efd6-7fecf7906331",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Pretrained model\n",
    "pre_trained_model = InceptionV3(input_shape = (150,150,3),# Shape of our images\n",
    "                                include_top = False,# Leave out the last fully connected layer\n",
    "                                weights = 'imagenet'\n",
    "                               )"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v8Y7R_uSEfBn"
   },
   "source": [
    "# make all layers not trainable (The layers added later will still be trainable which we can see in the learning process)\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "43qP8O-WEfBq"
   },
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow import keras\n",
    "\n",
    "#Flatten the output layer to 1 dimension\n",
    "x = keras.layers.Flatten()(pre_trained_model.output)\n",
    "\n",
    "# Add a fully connected layer with 1024 hidden units an ReLU activation\n",
    "x = keras.layers.Dense(1024,activation='relu')(x)\n",
    "# Add a dropout rate of 0.2\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "# Add a final sigmoid layer for classification\n",
    "x = keras.layers.Dense(1,activation = 'sigmoid')(x)\n",
    "\n",
    "model = keras.Model(pre_trained_model.input, x)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer = RMSprop(lr=0.0001),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZeS5v_4ZEfBs"
   },
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bUlvpSsDEfBu"
   },
   "source": [
    "# import relevant packages\n",
    "import os \n",
    "import zipfile\n",
    "\n",
    "# Unzip the compressed file\n",
    "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip,'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "\n",
    "\n",
    "# create the relevant directories\n",
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_vats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UL7WbaLqEfBw",
    "outputId": "43cf7100-a3f4-4cb1-c596-e858400f7ce4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "# Create the train generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size = 20,\n",
    "                                                    class_mode = 'binary',\n",
    "                                                    target_size = (150,150))\n",
    "\n",
    "# Create the validation generator\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        batch_size = 20,\n",
    "                                                        class_mode = 'binary',\n",
    "                                                        target_size = (150,150))"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wZ-3_kOhEfBz"
   },
   "source": [
    "# Creating custom callback\n",
    "class myCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if(logs.get('acc') > 0.959):\n",
    "            print(\"\\nReached 99.9% accuracy so cancelling training!\") # This should say 95.9%\n",
    "            self.model.stop_training = True"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7B6D37uSEfB1",
    "outputId": "0ee7614d-d8a0-45c1-c8ce-0cd1d9fe797c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "callbacks = myCallback()\n",
    "\n",
    "# Running the learning.\n",
    "history = model.fit_generator(train_generator,\n",
    "                              validation_data = validation_generator,\n",
    "                              steps_per_epoch = 100,\n",
    "                              epochs = 100,\n",
    "                              validation_steps = 50,\n",
    "                              verbose = 2,\n",
    "                              callbacks = [callbacks])"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-cf6bc3fe0646>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "100/100 - 20s - loss: 1.3410 - acc: 0.8425 - val_loss: 0.2203 - val_acc: 0.9520\n",
      "Epoch 2/100\n",
      "100/100 - 19s - loss: 0.4838 - acc: 0.8930 - val_loss: 0.2660 - val_acc: 0.9340\n",
      "Epoch 3/100\n",
      "100/100 - 19s - loss: 0.4172 - acc: 0.8975 - val_loss: 0.1994 - val_acc: 0.9490\n",
      "Epoch 4/100\n",
      "100/100 - 19s - loss: 0.3724 - acc: 0.9085 - val_loss: 0.1760 - val_acc: 0.9530\n",
      "Epoch 5/100\n",
      "100/100 - 19s - loss: 0.3952 - acc: 0.9025 - val_loss: 0.1598 - val_acc: 0.9530\n",
      "Epoch 6/100\n",
      "100/100 - 19s - loss: 0.3496 - acc: 0.9085 - val_loss: 0.2077 - val_acc: 0.9490\n",
      "Epoch 7/100\n",
      "100/100 - 19s - loss: 0.3152 - acc: 0.9175 - val_loss: 0.2499 - val_acc: 0.9360\n",
      "Epoch 8/100\n",
      "100/100 - 19s - loss: 0.3606 - acc: 0.9125 - val_loss: 0.1484 - val_acc: 0.9600\n",
      "Epoch 9/100\n",
      "100/100 - 19s - loss: 0.2622 - acc: 0.9255 - val_loss: 0.3081 - val_acc: 0.9330\n",
      "Epoch 10/100\n",
      "100/100 - 19s - loss: 0.2680 - acc: 0.9255 - val_loss: 0.1744 - val_acc: 0.9510\n",
      "Epoch 11/100\n",
      "100/100 - 19s - loss: 0.3023 - acc: 0.9285 - val_loss: 0.1694 - val_acc: 0.9560\n",
      "Epoch 12/100\n",
      "100/100 - 19s - loss: 0.2605 - acc: 0.9265 - val_loss: 0.2238 - val_acc: 0.9440\n",
      "Epoch 13/100\n",
      "100/100 - 19s - loss: 0.2875 - acc: 0.9265 - val_loss: 0.4406 - val_acc: 0.9000\n",
      "Epoch 14/100\n",
      "100/100 - 19s - loss: 0.2507 - acc: 0.9260 - val_loss: 0.1775 - val_acc: 0.9590\n",
      "Epoch 15/100\n",
      "100/100 - 19s - loss: 0.2793 - acc: 0.9300 - val_loss: 0.5622 - val_acc: 0.9050\n",
      "Epoch 16/100\n",
      "100/100 - 19s - loss: 0.2564 - acc: 0.9340 - val_loss: 0.1566 - val_acc: 0.9600\n",
      "Epoch 17/100\n",
      "100/100 - 19s - loss: 0.2497 - acc: 0.9320 - val_loss: 0.1566 - val_acc: 0.9570\n",
      "Epoch 18/100\n",
      "100/100 - 19s - loss: 0.2462 - acc: 0.9335 - val_loss: 0.3003 - val_acc: 0.9390\n",
      "Epoch 19/100\n",
      "100/100 - 19s - loss: 0.2532 - acc: 0.9315 - val_loss: 0.1141 - val_acc: 0.9600\n",
      "Epoch 20/100\n",
      "100/100 - 19s - loss: 0.1973 - acc: 0.9360 - val_loss: 0.1561 - val_acc: 0.9500\n",
      "Epoch 21/100\n",
      "100/100 - 19s - loss: 0.2076 - acc: 0.9385 - val_loss: 0.1568 - val_acc: 0.9570\n",
      "Epoch 22/100\n",
      "100/100 - 19s - loss: 0.1946 - acc: 0.9475 - val_loss: 0.1336 - val_acc: 0.9620\n",
      "Epoch 23/100\n",
      "100/100 - 19s - loss: 0.1885 - acc: 0.9440 - val_loss: 0.1742 - val_acc: 0.9640\n",
      "Epoch 24/100\n",
      "100/100 - 19s - loss: 0.1986 - acc: 0.9450 - val_loss: 0.2106 - val_acc: 0.9460\n",
      "Epoch 25/100\n",
      "100/100 - 19s - loss: 0.2306 - acc: 0.9375 - val_loss: 0.1464 - val_acc: 0.9660\n",
      "Epoch 26/100\n",
      "100/100 - 19s - loss: 0.2117 - acc: 0.9445 - val_loss: 0.1540 - val_acc: 0.9600\n",
      "Epoch 27/100\n",
      "100/100 - 19s - loss: 0.1953 - acc: 0.9510 - val_loss: 0.3095 - val_acc: 0.9400\n",
      "Epoch 28/100\n",
      "100/100 - 19s - loss: 0.2399 - acc: 0.9430 - val_loss: 0.1498 - val_acc: 0.9630\n",
      "Epoch 29/100\n",
      "100/100 - 19s - loss: 0.2114 - acc: 0.9445 - val_loss: 0.1454 - val_acc: 0.9670\n",
      "Epoch 30/100\n",
      "100/100 - 19s - loss: 0.2084 - acc: 0.9475 - val_loss: 0.1366 - val_acc: 0.9660\n",
      "Epoch 31/100\n",
      "100/100 - 19s - loss: 0.2225 - acc: 0.9455 - val_loss: 0.1741 - val_acc: 0.9560\n",
      "Epoch 32/100\n",
      "100/100 - 19s - loss: 0.1723 - acc: 0.9545 - val_loss: 0.1628 - val_acc: 0.9620\n",
      "Epoch 33/100\n",
      "100/100 - 19s - loss: 0.1843 - acc: 0.9515 - val_loss: 0.2107 - val_acc: 0.9540\n",
      "Epoch 34/100\n",
      "100/100 - 19s - loss: 0.1746 - acc: 0.9580 - val_loss: 0.1673 - val_acc: 0.9700\n",
      "Epoch 35/100\n",
      "100/100 - 19s - loss: 0.1817 - acc: 0.9540 - val_loss: 0.1629 - val_acc: 0.9630\n",
      "Epoch 36/100\n",
      "100/100 - 19s - loss: 0.1923 - acc: 0.9525 - val_loss: 0.1918 - val_acc: 0.9530\n",
      "Epoch 37/100\n",
      "100/100 - 19s - loss: 0.1799 - acc: 0.9575 - val_loss: 0.1897 - val_acc: 0.9580\n",
      "Epoch 38/100\n",
      "100/100 - 19s - loss: 0.1911 - acc: 0.9500 - val_loss: 0.1601 - val_acc: 0.9560\n",
      "Epoch 39/100\n",
      "100/100 - 19s - loss: 0.1421 - acc: 0.9575 - val_loss: 0.1900 - val_acc: 0.9610\n",
      "Epoch 40/100\n",
      "100/100 - 19s - loss: 0.1978 - acc: 0.9495 - val_loss: 0.1680 - val_acc: 0.9650\n",
      "Epoch 41/100\n",
      "\n",
      "Reached 99.9% accuracy so cancelling training!\n",
      "100/100 - 19s - loss: 0.1297 - acc: 0.9615 - val_loss: 0.2122 - val_acc: 0.9580\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dXRc_SxkQ8Qd"
   },
   "source": [
    "# Sources:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjgtYl0ARciy"
   },
   "source": [
    "Sources:\n",
    "\n",
    "NVIDIA WORKSHOP - there wasa  part on transfered learning which proved useful\n",
    "\n",
    "https://medium.com/analytics-vidhya/end-to-end-image-classification-project-using-tensorflow-46e78298fa2f\n",
    "\n",
    "https://medium.com/analytics-vidhya/transfer-learning-using-inception-v3-for-image-classification-86700411251b\n",
    "\n",
    "\n"
   ]
  }
 ]
}